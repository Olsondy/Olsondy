#!/usr/bin/env python3
"""
ç½‘ç«™å†…é“¾æœ‰æ•ˆæ€§æ£€æŸ¥å·¥å…·

ç”¨æ³•:
    python check-links.py                    # æ£€æŸ¥æœ¬åœ°å¼€å‘æœåŠ¡å™¨
    python check-links.py --url https://example.com
    python check-links.py --verbose          # æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
"""

import argparse
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import Set, List, Dict
from urllib.parse import urljoin, urlparse

import requests


@dataclass
class LinkResult:
    """é“¾æ¥æ£€æŸ¥ç»“æœ"""
    url: str
    status: int
    from_page: str
    is_valid: bool
    error: str = ""


@dataclass
class PageResult:
    """é¡µé¢æ£€æŸ¥ç»“æœ"""
    url: str
    total_links: int = 0
    valid_links: int = 0
    invalid_links: int = 0
    link_details: List[LinkResult] = field(default_factory=list)


class LinkChecker:
    """å†…é“¾æ£€æŸ¥å™¨"""

    def __init__(self, base_url: str, max_workers: int = 10, timeout: int = 10):
        self.base_url = base_url.rstrip("/")
        self.max_workers = max_workers
        self.timeout = timeout
        self.visited_urls: Set[str] = set()
        self.results: Dict[str, PageResult] = {}
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })

    def is_internal_link(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå†…é“¾"""
        parsed = urlparse(url)
        base_parsed = urlparse(self.base_url)

        # ç›¸å¯¹è·¯å¾„è‚¯å®šæ˜¯å†…é“¾
        if not parsed.netloc:
            return True

        # åŒåŸŸå
        return parsed.netloc == base_parsed.netloc

    def normalize_url(self, url: str, base: str) -> str:
        """æ ‡å‡†åŒ–URL"""
        if url.startswith(("//",)):
            return f"https:{url}"
        if url.startswith("/"):
            return f"{base}{url}"
        if not url.startswith("http"):
            return f"{base}/{url}"
        return url

    def extract_links(self, html: str, page_url: str) -> List[str]:
        """ä»HTMLä¸­æå–æ‰€æœ‰é“¾æ¥ï¼ˆæ”¯æŒæ ‡å‡†HTMLã€RSC payloadã€Markdownæ ¼å¼ï¼‰"""
        raw_links = []
        
        # 1. åŒ¹é…æ ‡å‡† HTML href å±æ€§
        href_pattern = r'href=["\'"]([^"\']+)["\'"]'
        raw_links.extend(re.findall(href_pattern, html, re.IGNORECASE))
        
        # 2. åŒ¹é… RSC payload / JSON ä¸­çš„å†…éƒ¨è·¯å¾„
        #    ä¾‹å¦‚: "/creation-lab/prompts/p003-emotional-underscore"
        rsc_pattern = r'["\'](/creation-lab/[a-zA-Z0-9\-/_]+)["\']'
        raw_links.extend(re.findall(rsc_pattern, html))
        
        # 3. åŒ¹é… Markdown é“¾æ¥æ ¼å¼ [text](url)
        md_pattern = r'\]\((/[^)\s]+)\)'
        raw_links.extend(re.findall(md_pattern, html))
        
        # 4. åŒ¹é…è½¬ä¹‰çš„ JSON è·¯å¾„ (å¦‚ RSC payload ä¸­çš„ \"\/creation-lab\/...\" )
        escaped_pattern = r'\\/creation-lab\\/[a-zA-Z0-9_/-]+'
        escaped_matches = re.findall(escaped_pattern, html)
        for match in escaped_matches:
            # å»æ‰è½¬ä¹‰æ–œæ 
            raw_links.append(match.replace('\\/', '/'))

        # è¿‡æ»¤å’Œæ ‡å‡†åŒ–
        links = []
        skip_extensions = [".pdf", ".zip", ".exe", ".dmg", ".css", ".js", ".png", ".jpg", ".jpeg", ".gif", ".svg", ".woff", ".woff2", ".ico"]
        
        for link in raw_links:
            # è·³è¿‡é”šç‚¹ã€telã€mailtoã€javascript
            if link.startswith(("#", "tel:", "mailto:", "javascript:", "data:")):
                continue
            # è·³è¿‡é™æ€èµ„æºå’Œæ–‡ä»¶ä¸‹è½½é“¾æ¥
            if any(ext in link.lower() for ext in skip_extensions):
                continue
            # è·³è¿‡ API è·¯å¾„
            if "/api/" in link:
                continue
            links.append(self.normalize_url(link, self.base_url))

        return list(set(links))  # å»é‡

    def check_link(self, url: str, from_page: str) -> LinkResult:
        """æ£€æŸ¥å•ä¸ªé“¾æ¥"""
        try:
            response = self.session.head(url, timeout=self.timeout, allow_redirects=True)
            # HEAD ä¸æ”¯æŒåˆ™å°è¯• GET
            if response.status_code == 405:
                response = self.session.get(url, timeout=self.timeout, allow_redirects=True)

            return LinkResult(
                url=url,
                status=response.status_code,
                from_page=from_page,
                is_valid=response.status_code < 400,
                error=""
            )
        except requests.Timeout:
            return LinkResult(
                url=url,
                status=0,
                from_page=from_page,
                is_valid=False,
                error="Timeout"
            )
        except requests.RequestException as e:
            return LinkResult(
                url=url,
                status=0,
                from_page=from_page,
                is_valid=False,
                error=str(e)
            )

    def scan_page(self, url: str) -> PageResult:
        """æ‰«æå•ä¸ªé¡µé¢"""
        print(f"æ‰«æ: {url}")

        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()

            links = self.extract_links(response.text, url)
            internal_links = [l for l in links if self.is_internal_link(l)]

            page_result = PageResult(url=url, total_links=len(internal_links))

            # å¹¶å‘æ£€æŸ¥é“¾æ¥
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self.check_link, link, url): link
                    for link in internal_links
                }
                for future in as_completed(futures):
                    result = future.result()
                    page_result.link_details.append(result)
                    if result.is_valid:
                        page_result.valid_links += 1
                    else:
                        page_result.invalid_links += 1

            self.results[url] = page_result
            return page_result

        except requests.RequestException as e:
            print(f"  é”™è¯¯: {e}", file=sys.stderr)
            return PageResult(url=url)

    def run(self, start_path: str = "/") -> Dict[str, PageResult]:
        """è¿è¡Œæ£€æŸ¥"""
        start_url = f"{self.base_url}{start_path}" if start_path != "/" else self.base_url
        self.visited_urls.add(start_url)

        # ç®€å•å¹¿åº¦ä¼˜å…ˆçˆ¬å–
        to_visit = [start_url]
        max_pages = 50  # é™åˆ¶é¡µé¢æ•°é‡é˜²æ­¢æ— é™å¾ªç¯

        while to_visit and len(self.visited_urls) < max_pages:
            current_url = to_visit.pop(0)
            if current_url in self.results:
                continue

            page_result = self.scan_page(current_url)

            # æ”¶é›†æ–°å‘ç°çš„é¡µé¢
            for link_result in page_result.link_details:
                if link_result.is_valid and link_result.url not in self.visited_urls:
                    self.visited_urls.add(link_result.url)
                    # åªæ·»åŠ çœ‹èµ·æ¥æ˜¯é¡µé¢çš„é“¾æ¥ï¼ˆéé™æ€èµ„æºï¼‰
                    if not any(ext in link_result.url for ext in [".css", ".js", ".png", ".jpg", ".svg", ".woff", ".ico"]):
                        to_visit.append(link_result.url)

        return self.results

    def print_report(self, verbose: bool = False):
        """æ‰“å°æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("å†…é“¾æ£€æŸ¥æŠ¥å‘Š")
        print("=" * 60)

        total_pages = len(self.results)
        total_valid = sum(r.valid_links for r in self.results.values())
        total_invalid = sum(r.invalid_links for r in self.results.values())

        print(f"\næ‰«æé¡µé¢æ•°: {total_pages}")
        print(f"æœ‰æ•ˆé“¾æ¥: {total_valid}")
        print(f"æ— æ•ˆé“¾æ¥: {total_invalid}")
        if total_valid + total_invalid > 0:
            print(f"æœ‰æ•ˆç‡: {total_valid / (total_valid + total_invalid) * 100:.1f}%")

        if total_invalid > 0:
            print("\næ— æ•ˆé“¾æ¥åˆ—è¡¨:")
            print("-" * 60)

            for page_url, page_result in self.results.items():
                invalid = [l for l in page_result.link_details if not l.is_valid]
                if invalid:
                    print(f"\né¡µé¢: {page_url}")
                    for link in invalid:
                        if link.error:
                            print(f"  âŒ {link.url} ({link.status}) - {link.error}")
                        else:
                            print(f"  âŒ {link.url} ({link.status})")

        if verbose:
            print("\n" + "=" * 60)
            print("è¯¦ç»†æŠ¥å‘Š:")
            print("=" * 60)

            for page_url, page_result in sorted(self.results.items()):
                print(f"\nğŸ“„ {page_url}")
                print(f"   æ€»é“¾æ¥: {page_result.total_links}")
                print(f"   æœ‰æ•ˆ: {page_result.valid_links} | æ— æ•ˆ: {page_result.invalid_links}")

                if page_result.link_details:
                    for link in sorted(page_result.link_details, key=lambda x: x.url):
                        status_icon = "âœ…" if link.is_valid else "âŒ"
                        print(f"   {status_icon} {link.url} [{link.status}]")


def main():
    parser = argparse.ArgumentParser(description="æ£€æŸ¥ç½‘ç«™å†…é“¾æœ‰æ•ˆæ€§")
    parser.add_argument("--url", default="http://localhost:3000", help="ç½‘ç«™URL (é»˜è®¤: http://localhost:3000)")
    parser.add_argument("--start", default="/", help="èµ·å§‹è·¯å¾„ (é»˜è®¤: /)")
    parser.add_argument("--workers", type=int, default=10, help="å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 10)")
    parser.add_argument("--timeout", type=int, default=10, help="è¯·æ±‚è¶…æ—¶ç§’æ•° (é»˜è®¤: 10)")
    parser.add_argument("-v", "--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    print(f"å¼€å§‹æ£€æŸ¥: {args.url}")

    checker = LinkChecker(
        base_url=args.url,
        max_workers=args.workers,
        timeout=args.timeout
    )

    checker.run(start_path=args.start)
    checker.print_report(verbose=args.verbose)


if __name__ == "__main__":
    main()
